import pickle
from pathlib import Path
import re
import os
import joblib
import string
import requests
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


class RagSystem:
    def __init__(self, api_key: str, docs_dir: str = "docs"):
        self.api_key = api_key
        self.docs_dir = Path(docs_dir)
        self.documents = []  # åŸå§‹æ–‡æ¡£
        self.doc_chunks = []  # æ–‡æ¡£åˆ†å—
        self.vectorizer = None
        self.doc_vectors = None  # å‘é‡çŸ©é˜µ
        self.stopwords = self.load_stopwords()

        # ç¼“å­˜ç›®å½•
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)

        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.chunks_cache = self.cache_dir / "doc_chunks.pkl"
        self.vectorizer_cache = self.cache_dir / "vectorizer_cache.pkl"
        self.vector_matrix_cache = self.cache_dir / "vector_matrix_cache.pkl"

        # SiliconFlow API endpoint
        self.api_url = "https://api.siliconflow.cn/v1/chat/completions"

    # ------------------ åˆå§‹åŒ– ------------------
    def initialize(self):
        print("ğŸ”§ æ™ºèƒ½å›¾ä¹¦æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–ä¸­...")
        self.load_documents()
        self.process_documents()
        self.build_vector_index()
        print("âœ… åˆå§‹åŒ–å®Œæˆï¼Œå¯ä»¥å¼€å§‹æé—®ï¼")

    # ------------------ æ–‡æ¡£åŠ è½½ ------------------
    def load_documents(self):
        print("ğŸ“˜ æ­£åœ¨åŠ è½½èµ„æ–™æ–‡æ¡£...")
        if not self.docs_dir.exists() or not self.docs_dir.is_dir():
            raise FileNotFoundError(f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {self.docs_dir.absolute()}")

        for file_path in self.docs_dir.glob("*.txt"):
            print(f"æ‰¾åˆ°æ–‡æ¡£: {file_path}")
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read().strip()
                    if content:
                        self.documents.append({
                            "filename": file_path.name,
                            "content": content,
                            "path": str(file_path)
                        })
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        print(f"âœ… å…±åŠ è½½ {len(self.documents)} ä¸ªæ–‡æ¡£ã€‚")

    # ------------------ æ–‡æœ¬åˆ†å— ------------------
    def split_text_chunks(self, text: str, chunk_size: int = 300):
        """å°†æ–‡æœ¬æŒ‰å¥å·åˆ†å—"""
        sentences = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ])", text)
        sentences = [s.strip() for s in sentences if s.strip()]
        chunks, current_chunk = [], ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        if current_chunk:
            chunks.append(current_chunk.strip())
        return chunks

    # ------------------ æ–‡æ¡£é¢„å¤„ç†ï¼ˆå¸¦ç¼“å­˜ï¼‰ ------------------
    def process_documents(self):
        if self.chunks_cache.exists():
            print("ğŸ’¾ å‘ç°æ–‡æ¡£åˆ†å—ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½...")
            with open(self.chunks_cache, "rb") as f:
                self.doc_chunks = pickle.load(f)
            print(f"âœ… ä»ç¼“å­˜ä¸­åŠ è½½äº† {len(self.doc_chunks)} ä¸ªåˆ†å—ã€‚")
            return

        print("ğŸ§© å¼€å§‹å¤„ç†æ–‡æ¡£åˆ†å—...")
        for doc in self.documents:
            chunks = self.split_text_chunks(doc["content"])
            for i, chunk in enumerate(chunks):
                self.doc_chunks.append({
                    "content": chunk,
                    "source": doc["filename"],
                    "chunk_id": i,
                    "full_path": doc["path"]
                })

        with open(self.chunks_cache, "wb") as f:
            pickle.dump(self.doc_chunks, f)
        print(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(self.doc_chunks)} ä¸ªchunkã€‚")

    # ------------------ åœç”¨è¯åŠ è½½ ------------------
    def load_stopwords(self):
        stopwords_file = Path("ä¸­æ–‡åœç”¨è¯åº“.txt")
        stopwords = set()
        if stopwords_file.exists():
            with open(stopwords_file, "r", encoding="utf-8") as f:
                stopwords = {line.strip() for line in f if line.strip()}
            print(f"âœ… å·²åŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯ã€‚")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°åœç”¨è¯åº“æ–‡ä»¶ï¼Œå°†ä½¿ç”¨é»˜è®¤åœç”¨è¯è¡¨ã€‚")
            stopwords.update({
                "çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "æˆ‘", "æœ‰", "å°±", "ä¸", "äºº",
                "éƒ½", "ä¸€ä¸ª", "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»",
                "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "è‡ªå·±", "è¿™", "é‚£", "è¿˜", "ä»€ä¹ˆ"
            })
        return stopwords

    # ------------------ ä¸­æ–‡åˆ†è¯ ------------------
    def chinese_tokenizer(self, text: str):
        words = list(jieba.cut(text))
        cleaned_words = []
        for word in words:
            word = word.strip()
            if not word or word in self.stopwords:
                continue
            if word in string.punctuation or re.match(r"^[\W_]+$", word):
                continue
            if len(word) == 1:
                continue
            cleaned_words.append(word)
        return cleaned_words

    # ------------------ TF-IDF å‘é‡ç´¢å¼• ------------------
    def build_vector_index(self):
        """æ„å»ºæˆ–åŠ è½½ TF-IDF å‘é‡ç´¢å¼•"""
        if self.vectorizer_cache.exists() and self.vector_matrix_cache.exists():
            print("ğŸ’¾ æ£€æµ‹åˆ°å‘é‡ç¼“å­˜æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")
            self.vectorizer = joblib.load(self.vectorizer_cache)
            self.doc_vectors = joblib.load(self.vector_matrix_cache)
            print(f"âœ… ä»ç¼“å­˜åŠ è½½ TF-IDF çŸ©é˜µï¼Œå½¢çŠ¶: {self.doc_vectors.shape}")
            return self.doc_vectors

        print("âš™ï¸ æ­£åœ¨æ„å»º TF-IDF å‘é‡ç´¢å¼•...")
        corpus = [chunk["content"] for chunk in self.doc_chunks]
        self.vectorizer = TfidfVectorizer(
            tokenizer=self.chinese_tokenizer,
            token_pattern=None,
            max_features=None,
            min_df=1,
            max_df=0.95,
            ngram_range=(1, 2)
        )
        self.doc_vectors = self.vectorizer.fit_transform(corpus)

        joblib.dump(self.vectorizer, self.vectorizer_cache)
        joblib.dump(self.doc_vectors, self.vector_matrix_cache)
        print(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼å½¢çŠ¶: {self.doc_vectors.shape}")
        return self.doc_vectors

    # ------------------ ä»ç¼“å­˜åŠ è½½ ------------------
    def load_vector_cache(self):
        """ä»ç¼“å­˜åŠ è½½å‘é‡åŒ–æ¨¡å‹ä¸çŸ©é˜µ"""
        if not (self.vectorizer_cache.exists() and self.vector_matrix_cache.exists()):
            print("âš ï¸ æœªæ£€æµ‹åˆ°å‘é‡ç¼“å­˜æ–‡ä»¶ï¼Œè¯·å…ˆè°ƒç”¨ build_vector_index() æ„å»ºç´¢å¼•ã€‚")
            return False
        try:
            self.vectorizer = joblib.load(self.vectorizer_cache)
            self.doc_vectors = joblib.load(self.vector_matrix_cache)
            print(f"âœ… ä»ç¼“å­˜åŠ è½½ TF-IDF çŸ©é˜µï¼Œå½¢çŠ¶: {self.doc_vectors.shape}")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return False

    # ------------------ æ–‡æ¡£æ£€ç´¢ ------------------
    def search_chunks(self, query: str, top_k: int = 10, similarity_threshold: float = 0.05):
        if self.vectorizer is None or self.doc_vectors is None:
            raise ValueError("å‘é‡ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ build_vector_index()")

        query_vector = self.vectorizer.transform([query])
        similarity = cosine_similarity(query_vector, self.doc_vectors).flatten()
        valid_index = np.where(similarity > similarity_threshold)[0]
        if len(valid_index) == 0:
            return []

        sorted_index = valid_index[np.argsort(similarity[valid_index])[::-1]]
        top_index = sorted_index[:top_k]

        results = []
        for idx in top_index:
            chunk = self.doc_chunks[idx].copy()
            chunk["similarity"] = float(similarity[idx])
            results.append(chunk)
        return results

    # ------------------ SiliconFlow æ¨¡å‹è°ƒç”¨ ------------------
    def generate_answer(self, query: str, context_chunks):
        context = "\n\n".join([f"æ–‡æ¡£ç‰‡æ®µ{i + 1}: {chunk['content']}" for i, chunk in enumerate(context_chunks)])
        user_prompt = f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n{context}\n\né—®é¢˜ï¼š{query}"

        payload = {
            "model": "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡å­¦åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": user_prompt}
            ],
            "stream": False
        }

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        try:
            response = requests.post(self.api_url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                return result['choices'][0]['message']['content']
            else:
                return str(result)
        except requests.exceptions.RequestException as e:
            return f"âŒ è°ƒç”¨ SiliconFlow API å‡ºé”™: {e}"

    # ------------------ ä¸»é—®ç­”æµç¨‹ ------------------
    def ask(self, question: str):
        relevant_chunks = self.search_chunks(question)
        if not relevant_chunks:
            return {
                "question": question,
                "answer": "æŠ±æ­‰ï¼Œåœ¨ç›¸å…³æ–‡æ¡£ä¸­æ— æ³•æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚",
                "source": []
            }
        print(f"æ‰¾åˆ° {len(relevant_chunks)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œæ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.generate_answer(question, relevant_chunks)
        return {"question": question, "answer": answer, "source": relevant_chunks}


# ------------------ ä¸»å‡½æ•° ------------------
def main():
    rag = RagSystem("sk-ukzszmjmdpsurolcgrjhlhfgrnqvljaczcfgldezhvhkxsvg")
    rag.initialize()

    print("=== æ™ºèƒ½å›¾ä¹¦é—®ç­”ç³»ç»Ÿ ===")
    print("è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºã€‚")

    while True:
        question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š").strip()
        if question.lower() in ["quit", "exit", "é€€å‡º"]:
            print("æ„Ÿè°¢ä½¿ç”¨ç³»ç»Ÿï¼Œå†è§ï¼")
            break
        if not question:
            continue

        result = rag.ask(question)
        print("\n--- ç­”æ¡ˆ ---")
        print(result["answer"])
        print("\n--- æ¥æº ---")
        for src in result["source"]:
            print(f"- {src['source']} (chunk {src['chunk_id']}, ç›¸ä¼¼åº¦: {src['similarity']:.3f})")
        print("\n")


if __name__ == "__main__":
    main()
